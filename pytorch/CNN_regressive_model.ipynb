{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_regressive_model.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drvoss/Colab-Notebooks/blob/master/CNN_regressive_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "CALQnVV-zYva",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget http://vis-www.cs.umass.edu/lfw/lfw-deepfunneled.tgz\n",
        "!tar -zxvf lfw-deepfunneled.tgz\n",
        "!mkdir lfw-deepfunneled/train\n",
        "!mv lfw-deepfunneled/[A-W]* lfw-deepfunneled/train\n",
        "!mkdir lfw-deepfunneled/test\n",
        "!mv lfw-deepfunneled/[X-Z]* lfw-deepfunneled/test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8FYRESM11P6R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import (Dataset,DataLoader,TensorDataset)\n",
        "import tqdm\n",
        "\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision import transforms\n",
        "\n",
        "class DownSizedPairImageFolder(ImageFolder):\n",
        "  def __init__(self, root, transform=None,\n",
        "               large_size=128, small_size=32, **kwds):\n",
        "    super().__init__(root, transform=transform, **kwds)\n",
        "    self.large_resizer = transforms.Resize(large_size)\n",
        "    self.small_resizer = transforms.Resize(small_size)\n",
        "\n",
        "def __getitem__(self, index):\n",
        "  path, _ = self.imgs[index]\n",
        "  img = self.loader(path)\n",
        "  # 읽은 이미지를 128×128픽셀과 32×32픽셀로 리사이즈\n",
        "  large_img = self.large_resizer(img)\n",
        "  small_img = self.small_resizer(img)\n",
        "  # 기타 변환 적용\n",
        "  if self.transform is not None:\n",
        "    large_img = self.transform(large_img)\n",
        "    small_img = self.transform(small_img)\n",
        "  # 32픽셀의 이미지와 128픽셀의 이미지 반환\n",
        "  return small_img, large_img\n",
        "\n",
        "train_data = DownSizedPairImageFolder(\"/content/lfw-deepfunneled/train\",\n",
        "transform=transforms.ToTensor())\n",
        "test_data = DownSizedPairImageFolder(\"/content/lfw-deepfunneled/test\",\n",
        "transform=transforms.ToTensor())\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "net = nn.Sequential(\n",
        "  nn.Conv2d(3, 256, 4, stride=2, padding=1),\n",
        "  nn.ReLU(),\n",
        "  nn.BatchNorm2d(256),\n",
        "  nn.Conv2d(256, 512, 4, stride=2, padding=1),\n",
        "  nn.ReLU(),\n",
        "  nn.BatchNorm2d(512),\n",
        "  nn.ConvTranspose2d(512, 256, 4, stride=2, padding=1),\n",
        "  nn.ReLU(),\n",
        "  nn.BatchNorm2d(256),\n",
        "  nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1),\n",
        "  nn.ReLU(),\n",
        "  nn.BatchNorm2d(128),\n",
        "  nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),\n",
        "  nn.ReLU(),\n",
        "  nn.BatchNorm2d(64),\n",
        "  nn.ConvTranspose2d(64, 3, 4, stride=2, padding=1)\n",
        ")\n",
        "\n",
        "import math\n",
        "def psnr(mse, max_v=1.0):\n",
        "  return 10 * math.log10(max_v**2 / mse)\n",
        "\n",
        "# 평가 헬퍼 함수\n",
        "def eval_net(net, data_loader, device=\"cpu\"):\n",
        "  # Dropout 및 BatchNorm을 무효화\n",
        "  net.eval()\n",
        "  ys = []\n",
        "  ypreds = []\n",
        "  for x, y in data_loader:\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    with torch.no_grad():\n",
        "      y_pred = net(x)\n",
        "    ys.append(y)\n",
        "    ypreds.append(y_pred)\n",
        "  # 미니 배치 단위로 예측 결과 등을 하나로 모은다\n",
        "  ys = torch.cat(ys)\n",
        "  ypreds = torch.cat(ypreds)\n",
        "  # 예측 정확도(MSE) 계산\n",
        "  score = nn.functional.mse_loss(ypreds, ys).item()\n",
        "  return score\n",
        "\n",
        "# 훈련 헬퍼 함수\n",
        "def train_net(net, train_loader, test_loader, optimizer_cls=optim.Adam\n",
        "              , loss_fn=nn.MSELoss()\n",
        "              , n_iter=10, device=\"cpu\"):\n",
        "  train_losses = []\n",
        "  train_acc = []\n",
        "  val_acc = []\n",
        "  optimizer = optimizer_cls(net.parameters())\n",
        "  for epoch in range(n_iter):\n",
        "    running_loss = 0.0\n",
        "    # 신경망을 훈련 모드로 설정\n",
        "    net.train()\n",
        "    n = 0\n",
        "    score = 0\n",
        "    # 시간이 많이 걸리므로 tqdm를 이용해서\n",
        "    # 진행 바 표시\n",
        "    for i, (xx, yy) in tqdm.tqdm(enumerate(train_loader),\n",
        "      total=len(train_loader)):\n",
        "      xx = xx.to(device)\n",
        "      yy = yy.to(device)\n",
        "      y_pred = net(xx)\n",
        "      loss = loss_fn(y_pred, yy)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      running_loss += loss.item()\n",
        "      n += len(xx)\n",
        "    train_losses.append(running_loss / len(train_loader))\n",
        "    # 검증 데이터의 훈련 정확도\n",
        "    val_acc.append(eval_net(net, test_loader, device))\n",
        "    # epoch의 결과 표시\n",
        "    print(epoch, train_losses[-1],\n",
        "    psnr(train_losses[-1]), psnr(val_acc[-1]), flush=True)\n",
        "    \n",
        "net.to(\"cuda:0\")\n",
        "train_net(net, train_loader, test_loader, device=\"cuda:0\")\n",
        "\n",
        "from torchvision.utils import save_image\n",
        "# 테스트 데이터로부터 랜덤으로 4개씩 추출하는 DataLoader\n",
        "random_test_loader = DataLoader(test_data, batch_size=4, shuffle=True)\n",
        "# DataLoader를 파이썬의 이터레이터로 변환해서 4개의 예로 추출\n",
        "it = iter(random_test_loader)\n",
        "x, y = next(it)\n",
        "# Bilinear를 사용한 확대\n",
        "bl_recon = torch.nn.functional.upsample(x, 128, mode=\"bilinear\", align_corners=True)\n",
        "# CNN으로 확대\n",
        "yp = net(x.to(\"cuda:0\")).to(\"cpu\")\n",
        "# torch.cat로 원본, Bilinear,CNN 이미지를 결합하고\n",
        "# save_image로 결합한 이미지를 출력(저장)\n",
        "save_image(torch.cat([y, bl_recon, yp], 0), \"cnn_upscale.jpg\", nrow=4)\n",
        "\n",
        "from IPython.display import Image,display_jpeg\n",
        "display_jpeg(Image('cnn_upscale.jpg'))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}